---
title: "Recognition of weight lifting activities using wearable sensors"
author: "Anton Ornatskyi"
date: "Wednesday, June 17, 2015"
output:
  html_document:
    keep_md: yes
---

#Synopsis
This report is covering selection of best prediction model for recognising weight lifting activity type using data gathered by sensors placed on athlete's body. In the research I had used data from original research performed by <http://groupware.les.inf.puc-rio.br/har>.

#Data Processing

##Loading and preprocessing the data

For faster processing, columns with more than 50% "NA" values were removed. Columns with date/time, athleteID and measurement window informaton were also removed. After that, data from main training set were partitioned as: training (60%), testing (20%) and validation (20%) sets.

```{r, echo=FALSE}
setwd("D:/BeagleBone/Workspace/RWorkspace/PMLCP")
#library(doParallel)
#cl <- makeCluster(detectCores())
#registerDoParallel(cl)
library(caret)
set.seed(256)
mainData<-read.csv("pml-training.csv", na.strings=c("NA","","#DIV/0!"))
score<-read.csv("pml-testing.csv", na.strings=c("NA","","#DIV/0!"))
score<-score[,colSums(is.na(mainData))<nrow(mainData)/2]
mainData <- mainData[,colSums(is.na(mainData))<nrow(mainData)/2]
mainData<-mainData[-c(1:7)]
score<-score[-c(1:7)]
inTrain<-createDataPartition(mainData$classe,p=0.6,list=FALSE)
training<-mainData[inTrain,]
testAndValidation<-mainData[-inTrain,]
inValidation<-createDataPartition(testAndValidation$classe,p=0.5,list=FALSE)
validation<-testAndValidation[inValidation,]
testing<-testAndValidation[-inValidation,]
usefulColumnsTrain<-nearZeroVar(training, saveMetrics=TRUE)
training<- training[,usefulColumnsTrain$nzv==FALSE & usefulColumnsTrain$zeroVar==FALSE]
dim(training)
```

##Models fitting
Ater performing zero covariates checking, we can see that rest of predictors have decent variability and can be used for basic model fitting. I'll check efficiency of two most popular methods: Random Forests and Boost. Firstly just trying to fit models with default parameters.

```{r, echo=FALSE,cache=TRUE}
#fitRFModel<-train(classe~.,method="rf",data=training)
#fitBoostModel<-train(classe~., method = "gbm",data=training, verbose = FALSE)
#fitControl<-trainControl(method="repeatedcv",number=5,repeats=5)
#gbmGrid <-  expand.grid(interaction.depth = c(5, 7, 9), n.trees = (3:10)*50, shrinkage = 0.1,n.minobsinnode = 10)
#fitRFBigModel<-train(classe~.,method="rf",data=training,trControl = fitControl)
#fitBoostBigModel<-train(classe~., method = "gbm",data=training,trControl = fitControl,tuneGrid = gbmGrid, verbose = FALSE)
#stopCluster(cl)
```

Default RF model use 52 predictors without preprocessing with Bootstraping as resampling mode. Best accuracy for this model was 0.98, and final value used for the model was mtry = 27
Default Boost model use the same train parameters and got a best accuracy in 0.96 with final values: n.trees = 150, interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10.

Secondly set trainControl and boost model parameters to fit computationly heavy models:
FitControl
```{r}
fitControl<-trainControl(method="repeatedcv",number=5,repeats=5)
```
GBMGrid
```{r}
gbmGrid <-  expand.grid(interaction.depth = c(5, 7, 9), n.trees = (3:10)*50, shrinkage = 0.1,n.minobsinnode = 10)
gbmGrid
```
I don't think I need preprocessing, because of decent accuracy gotten at default parameters

###Result of RF model fitting and using on the testing set:
Random Forest model
Resampling: Cross-Validated (5 fold, repeated 5 times) 

  mtry  Accuracy   Kappa      Accuracy SD  Kappa SD   
------ ---------- ----------- ------------ -----------  
   2    0.9886546  0.9856452  0.002460400  0.003114393
  27    0.9890964  0.9862056  0.002266613  0.002868355
  52    0.9823028  0.9776119  0.002519527  0.003189981

The final value used for the model was mtry = 27.

       A    B    C    D    E
  - ----  ---  ---  ---  ---
  A 1113    2    0    0    1
  B    5  751    1    1    1
  C    0    5  675    4    0
  D    0    1   12  628    2
  E    0    0    0    3  718

There is no significant accuracy increase in comparision with default model. It's consequences of very good default model accuracy.

###Result of Boost model fitting and using on the testing set:
Stochastic Gradient Boosting 
Resampling: Cross-Validated (5 fold, repeated 5 times) 

  interaction.depth  n.trees  Accuracy   Kappa      Accuracy SD  Kappa SD   
  -----------------  -------  ---------  ---------  -----------  -----------
  5                  150      0.9785154  0.9728216  0.003188722  0.004034524
  5                  200      0.9837972  0.9795033  0.003200228  0.004049257
  5                  250      0.9864126  0.9828125  0.003076150  0.003891164
  5                  300      0.9882299  0.9851109  0.002970559  0.003758044
  5                  350      0.9893678  0.9865507  0.002727805  0.003451195
  5                  400      0.9900131  0.9873669  0.002668693  0.003376242
  5                  450      0.9908793  0.9884627  0.002541377  0.003215154
  5                  500      0.9911341  0.9887849  0.002484977  0.003143768
  7                  150      0.9846465  0.9805780  0.002899403  0.003668921
  7                  200      0.9877035  0.9844451  0.002919517  0.003694462
  7                  250      0.9891301  0.9862496  0.002633961  0.003332874
  7                  300      0.9902341  0.9876465  0.002314127  0.002927835
  7                  350      0.9910323  0.9886559  0.002426512  0.003070388
  7                  400      0.9916097  0.9893864  0.002302559  0.002913703
  7                  450      0.9922892  0.9902460  0.002032638  0.002571989
  7                  500      0.9927137  0.9907829  0.002054950  0.002600293
  9                  150      0.9877883  0.9845527  0.002674406  0.003383491
  9                  200      0.9897585  0.9870450  0.002466707  0.003120630
  9                  250      0.9908456  0.9884199  0.002231482  0.002823185
  9                  300      0.9918306  0.9896658  0.001913167  0.002420812
  9                  350      0.9923230  0.9902887  0.002095973  0.002652018
  9                  400      0.9926627  0.9907184  0.001878771  0.002377359
  9                  450      0.9930703  0.9912340  0.002116573  0.002678130
  9                  500      0.9933930  0.9916423  0.002030751  0.002569448

Tuning parameter 'shrinkage' was held constant at a value of 0.1
Tuning parameter 'n.minobsinnode' was held constant at a value of 10
Accuracy was used to select the optimal model using  the largest value.
The final values used for the model were n.trees = 500, interaction.depth = 9, shrinkage = 0.1 and n.minobsinnode = 10.
   
       A    B    C    D    E
  - ----  ---  ---  ---  ---    
  A 1114    1    0    0    1
  B    1  758    0    0    0
  C    0    5  677    2    0
  D    0    1    7  634    1
  E    0    0    1    1  719
  
As we can see, best Boost model is overally best by Accuracy and Kappa. I will use it for final results prediction.

###Checking best model on out-of-sample error using validation set of data

Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
---------- ----  ---  ---  ---  ---
         A 1116    0    0    0    0
         B    4  752    3    0    0
         C    0    3  680    1    0
         D    0    0    3  640    0
         E    0    0    0    2  719

Overall Statistics
                                          
               Accuracy : 0.9959          
                 95% CI : (0.9934, 0.9977)
    No Information Rate : 0.2855          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9948          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
-------------------  -------- -------- -------- -------- --------                     
Sensitivity            0.9964   0.9960   0.9913   0.9953   1.0000
Specificity            1.0000   0.9978   0.9988   0.9991   0.9994
Pos Pred Value         1.0000   0.9908   0.9942   0.9953   0.9972
Neg Pred Value         0.9986   0.9991   0.9981   0.9991   1.0000
Prevalence             0.2855   0.1925   0.1749   0.1639   0.1833
Detection Rate         0.2845   0.1917   0.1733   0.1631   0.1833
Detection Prevalence   0.2845   0.1935   0.1744   0.1639   0.1838
Balanced Accuracy      0.9982   0.9969   0.9950   0.9972   0.9997

#Results
Best model for this kind of data is Boost based model with computational hungry parameters. But RF model with basic train control parameters can be used with small accuracy decrease. Also you can make model lighter using covariate creation and by removing less influential predictors from training.

